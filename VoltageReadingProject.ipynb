{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "# DISCRETE COURIER TRANSFORM \n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "from scipy.fft import idct\n",
        "from scipy.fft import dct\n",
        "\n",
        "# read the CSV file data\n",
        "real_data = pd.read_csv('/content/voltagereading.csv', parse_dates=['timestamp'])\n",
        "\n",
        "# extract the CSV file time series data\n",
        "timeseries_data = real_data['value'].values\n",
        "\n",
        "# apply Discrete Courier Transform function(inbuilt) to the extracted data\n",
        "dct_data = dct(timeseries_data)\n",
        "\n",
        "# set the last 90% of the coefficients as zero\n",
        "dctdata_size = len(dct_data)\n",
        "dctdata_zero = int(0.1 * dctdata_size)\n",
        "dct_data[dctdata_zero:] = 0\n",
        "\n",
        "# construct the time series data using the modified coefficients\n",
        "new_timeseries_data = idct(dct_data)\n",
        "\n",
        "# create a new DataFrame with the new time series data constructed\n",
        "transformed_data = pd.DataFrame({'timestamp': real_data['timestamp'], 'nodeid': real_data['nodeid'], 'source': real_data['source'], 'fqdd': real_data['fqdd'], 'value': new_timeseries_data})\n",
        "\n",
        "# change the newly constructed data to a new CSV file\n",
        "transformed_data.to_csv('transformed_data.csv', index=False)\n"
      ],
      "metadata": {
        "id": "EAibRnygzsp2"
      },
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# New Section"
      ],
      "metadata": {
        "id": "Q6cUDq_65n3-"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# New Section"
      ],
      "metadata": {
        "id": "Igwr2S-R5n6B"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "\n",
        "# read the first CSV file\n",
        "df1 = pd.read_csv('/content/voltagereading.csv')\n",
        "\n",
        "# read the second CSV file\n",
        "df2 = pd.read_csv('/content/transformed_data.csv')\n",
        "\n",
        "# get the number of rows and columns in each dataframe\n",
        "rows1, cols1 = df1.shape\n",
        "rows2, cols2 = df2.shape\n",
        "\n",
        "# calculate the total number of elements in each dataframe\n",
        "elements1 = rows1 * cols1\n",
        "elements2 = rows2 * cols2\n",
        "\n",
        "# compare the total number of elements in each dataframe\n",
        "if elements1 > elements2:\n",
        "    print('File 1 has a larger data volume than File 2' + str(elements1) + \" \" + str(elements2))\n",
        "elif elements1 < elements2:\n",
        "    print('File 2 has a larger data volume than File 1')\n",
        "else:\n",
        "    print('File 1 and File 2 have the same data volume')"
      ],
      "metadata": {
        "id": "ToLpZu8e19ja",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "34a8cce5-94bd-4a8e-973f-ee1b884bbc6e"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "File 1 has a larger data volume than File 21541215 1471310\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# New Section"
      ],
      "metadata": {
        "id": "eDQERwc_v7az"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#DOWNSAMPLING\n",
        "import pandas as pd\n",
        "import psutil\n",
        "import time\n",
        "\n",
        "# Get memory usage before compression\n",
        "downsampling_before_memory = psutil.virtual_memory().used\n",
        "\n",
        "# Record start time\n",
        "downsampling_start_time = time.time()\n",
        "\n",
        "# Record end time\n",
        "downsampling_end_time = time.time()\n",
        "\n",
        "# Start monitoring CPU usage\n",
        "psutil.cpu_percent(interval=1)\n",
        "\n",
        "# read the input real data\n",
        "real_data = pd.read_csv('/content/voltagereading.csv')\n",
        "\n",
        "# set the downsampling factor\n",
        "downsample_factor = 2\n",
        "\n",
        "# group the data by nodeid and fqdd\n",
        "grouped_data = real_data.groupby(['nodeid', 'fqdd'])\n",
        "\n",
        "# take the mean of every n rows\n",
        "downsampled = grouped_data.apply(lambda x: x.reset_index(drop=True).iloc[::downsample_factor].reset_index(drop=True))\n",
        "\n",
        "# save the downsampled data to a file\n",
        "downsampled.to_csv('downsampled_data.csv', index=False)\n",
        "\n",
        "cpu_usage = psutil.cpu_percent(interval=1, percpu=True)\n",
        "downsampling_cpu_usage = sum(cpu_usage) / len(cpu_usage)\n",
        "\n",
        "print(\"Downsampling CPU Usage : {:.2f}%\".format(downsampling_cpu_usage))\n",
        "\n",
        "# Get memory usage after compression\n",
        "downsampling_after_memory = psutil.virtual_memory().used\n",
        "\n",
        "# Calculate memory usage during compression\n",
        "memory_used_during_downsampling = (downsampling_after_memory - downsampling_before_memory)/(1024*1024)\n",
        "print('Downsampling Memory Usage in mega bytes : ', memory_used_during_downsampling)\n",
        "\n",
        "# Calculate processing time\n",
        "downsampling_processing_time = downsampling_end_time - downsampling_start_time\n",
        "print(\"Downsampling Processing Time : {:.7f} seconds\".format(downsampling_processing_time))"
      ],
      "metadata": {
        "id": "IDn7fWqaAqqK",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "fafc2c19-af68-4f98-c429-178ff656dd37"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Downsampling CPU Usage : 3.00%\n",
            "Downsampling Memory Usage in mega bytes :  20.48046875\n",
            "Downsampling Processing Time : 0.0000470 seconds\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# RUN LENGTH ENCODING\n",
        "import psutil\n",
        "import time\n",
        "import pandas as pd\n",
        "\n",
        "# Get memory usage before compression\n",
        "runlengthencoding_before_memory = psutil.virtual_memory().used\n",
        "\n",
        "# Record start time\n",
        "runlengthencoding_start_time = time.time()\n",
        "\n",
        "# Record end time\n",
        "runlengthencoding_end_time = time.time()\n",
        "\n",
        "# Start monitoring CPU usage\n",
        "psutil.cpu_percent(interval=1)\n",
        "\n",
        "# Read the input data from CSV file\n",
        "real_data = pd.read_csv('/content/voltagereading.csv')\n",
        "\n",
        "# Perform run-length encoding on the 'value' column for each node and source\n",
        "data_encoded = []\n",
        "for nodeid in real_data['nodeid'].unique():\n",
        "    for source in real_data['source'].unique():\n",
        "        values = real_data[(real_data['nodeid'] == nodeid) & (real_data['source'] == source)]['value']\n",
        "        values_encoded = []\n",
        "        count = 1\n",
        "        for i in range(1, len(values)):\n",
        "            if values.iloc[i] == values.iloc[i-1]:\n",
        "                count += 1\n",
        "            else:\n",
        "                values_encoded.append((values.iloc[i-1], count))\n",
        "                count = 1\n",
        "        values_encoded.append((values.iloc[-1], count))\n",
        "        data_encoded.append({\n",
        "            'nodeid': nodeid,\n",
        "            'source': source,\n",
        "            'encoded_values': values_encoded\n",
        "        })\n",
        "\n",
        "# Convert the encoded data into a DataFrame and save it to a CSV file\n",
        "data_encoded = pd.DataFrame(data_encoded)\n",
        "data_encoded.to_csv('runlengthencoded_data.csv', index=False)\n",
        "\n",
        "cpu_usage = psutil.cpu_percent(interval=1, percpu=True)\n",
        "runlengthencoding_cpu_usage = sum(cpu_usage) / len(cpu_usage)\n",
        "print(\"Run Length Encoding CPU Usage: {:.2f}%\".format(runlengthencoding_cpu_usage))\n",
        "\n",
        "# Get memory usage after compression\n",
        "runlengthencoding_after_memory = psutil.virtual_memory().used\n",
        "\n",
        "# Calculate memory usage during compression\n",
        "memory_used_during_runlengthencoding = (runlengthencoding_after_memory - runlengthencoding_before_memory)/(1024*1024)\n",
        "print('Run Length Encoding Memory Usage in mega bytes : ', memory_used_during_runlengthencoding)\n",
        "\n",
        "# Calculate processing time\n",
        "runlengthencoding_processing_time = runlengthencoding_end_time - runlengthencoding_start_time\n",
        "print(\"Run Length Encoding Processing Time: {:.7f} seconds\".format(runlengthencoding_processing_time))\n"
      ],
      "metadata": {
        "id": "rwnsgfss2Por",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "801eb079-38d4-41f7-c0dd-c23781c57b04"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Run Length Encoding CPU Usage: 4.55%\n",
            "Run Length Encoding Memory Usage in mega bytes :  31.828125\n",
            "Run Length Encoding Processing Time: 0.0000288 seconds\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import csv\n",
        "\n",
        "def run_length_decoding(input_file_path, output_file_path):\n",
        "    with open(input_file_path, newline='') as input_file:\n",
        "        with open(output_file_path, 'w', newline='') as output_file:\n",
        "            reader = csv.reader(input_file)\n",
        "            writer = csv.writer(output_file)\n",
        "            for row in reader:\n",
        "                decompressed_row = []\n",
        "                for value in row:\n",
        "                    if ':' in value:\n",
        "                        value_parts = value.split(':')\n",
        "                        count = int(value_parts[0])\n",
        "                        repeated_value = value_parts[1]\n",
        "                        decompressed_row += [repeated_value] * count\n",
        "                    else:\n",
        "                        decompressed_row.append(value)\n",
        "                writer.writerow(decompressed_row)\n",
        "run_length_decoding('runlengthencoded_data.csv', 'runlengthdecoded_data.csv')"
      ],
      "metadata": {
        "id": "NZBvmMPEZOfW"
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import csv\n",
        "\n",
        "def compare_csv_files(file1_path, file2_path):\n",
        "    # Read in the contents of both CSV files\n",
        "    with open(file1_path, newline='') as file1, open(file2_path, newline='') as file2:\n",
        "        file1_reader = csv.reader(file1)\n",
        "        file2_reader = csv.reader(file2)\n",
        "\n",
        "        # Step 2: Compare the two CSV files row by row\n",
        "        rows_match = True\n",
        "        row_idx = 0\n",
        "        while rows_match:\n",
        "            file1_row = next(file1_reader, None)\n",
        "            file2_row = next(file2_reader, None)\n",
        "\n",
        "            # If we have reached the end of one or both files, stop comparing\n",
        "            if file1_row is None or file2_row is None:\n",
        "                break\n",
        "\n",
        "            # If the rows don't match, print a message and stop comparing\n",
        "            if file1_row != file2_row:\n",
        "                print(\"The two CSV files differ at row\", row_idx, \":\")\n",
        "                print(file1_row)\n",
        "                print(file2_row)\n",
        "                rows_match = False\n",
        "\n",
        "            row_idx += 1\n",
        "\n",
        "        # If the files have a different number of rows, print a message\n",
        "        if rows_match is True and next(file1_reader, None) is not None:\n",
        "            print(\"The two CSV files have a different number of rows:\")\n",
        "            print(file1_path, \"has more rows\")\n",
        "        elif rows_match is True and next(file2_reader, None) is not None:\n",
        "            print(\"The two CSV files have a different number of rows:\")\n",
        "            print(file2_path, \"has more rows\")\n",
        "        else:\n",
        "            print(\"The two CSV files have the same rows and values.\")\n",
        "\n",
        "\n",
        "compare_csv_files('runlengthdecoded_data.csv', 'voltagereading.csv')\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "QznBCYq4aFbp",
        "outputId": "b2fde0d6-457f-40ee-90af-883442ce467d"
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "The two CSV files differ at row 0 :\n",
            "['nodeid', 'source', 'encoded_values']\n",
            "['timestamp', 'nodeid', 'source', 'fqdd', 'value']\n",
            "The two CSV files have the same rows and values.\n"
          ]
        }
      ]
    }
  ]
}